{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten, \n",
    "    Conv2D,\n",
    "    MaxPooling2D)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', '\\n', 'Peter', 'Blackburn', '\\n', 'BRUSSELS', '1996-08-22', '\\n', 'The', 'European', 'Commission', 'said']\n"
     ]
    }
   ],
   "source": [
    "# Data structure :'Word POS_tag syntactic_chunk_tag named_entity_tag \\n'\n",
    "\n",
    "# train = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.train', 'at')\n",
    "# testa = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.testa')\n",
    "# testb = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.testb')\n",
    "  \n",
    "def get_words(path, pos): # Add a 'r' before the path for raw string, pos between 0 and 3 see data structure\n",
    "    data = open(path)\n",
    "    list_words = []\n",
    "    new_data = data.readlines()[2:] # Skip first 2 lines\n",
    "    mylist = list(new_data)\n",
    "    for i in range(20): # TODO : CHANGES 20 TO len(mylist)\n",
    "        list_words.append((splitter(mylist[i], ' ', pos)))\n",
    "#     print(list_words)\n",
    "    return list_words\n",
    "\n",
    "def splitter(sentence, separator, pos): # pos between 0 and 3 for CONLL2003\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(separator):\n",
    "#         if word == '\\n':\n",
    "#             return\n",
    "        new_sentence.append(word)\n",
    "    return new_sentence[pos]\n",
    "\n",
    "def get_prefixe(list_words, length_prefixe):\n",
    "    list_prefixes = []\n",
    "    for word in list_words:\n",
    "#         if len(word) <= 3: # Conditional à vérifier niveau logique, maybe on prends les prefixes de tous les mots ...\n",
    "#             list_prefixes.append('')\n",
    "        prefixe = word[:length_prefixe]\n",
    "        list_prefixes.append(prefixe)\n",
    "    print('ha', list_prefixes)\n",
    "    return list_prefixes\n",
    "\n",
    "def get_suffixe(list_words, length_suffixe):\n",
    "    list_suffixes = []\n",
    "    for word in list_words:\n",
    "#         if len(word) <= 3: # Conditional à vérifier niveau logique, maybe on prends les prefixes de tous les mots ...\n",
    "#             list_prefixes.append('')\n",
    "        util_position = length_suffixe * (-1)\n",
    "        suffixe = word[util_position:]\n",
    "        list_suffixes.append(suffixe)\n",
    "    print('ho', list_suffixes)\n",
    "    return list_suffixes\n",
    "\n",
    "def get_capital(list_words):\n",
    "    # This function returns 2 lists : 1/ If the word have a capital and 2/ If the word is the 1st word of the sentence\n",
    "    list_capitals = []\n",
    "    list_first_word = []\n",
    "    flag_new_sentence = True\n",
    "    for word in list_words:\n",
    "        letter = word[0]\n",
    "        if letter.isupper():\n",
    "            list_capitals.append(1)\n",
    "            if flag_new_sentence:\n",
    "                list_first_word.append(1)\n",
    "                flag_ne\n",
    "                w_sentence = False\n",
    "        else:\n",
    "            list_capitals.append(0)\n",
    "        if word == '.': # OU '\\n' selon si on les enleve ou pas\n",
    "            flag_new_sentence = True\n",
    "    return (list_capitals, list_first_word)\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    list_of_words = get_words(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.train', 0)\n",
    "    print(list_of_words)\n",
    "#     list_prefixe_1 = get_prefixe(list_of_words, 1)\n",
    "#     list_prefixe_2 = get_prefixe(list_of_words, 2)\n",
    "#     list_prefixe_3 = get_prefixe(list_of_words, 3)\n",
    "#     list_suffixe_1 = get_suffixe(list_of_words, 1)\n",
    "#     list_suffixe_2 = get_suffixe(list_of_words, 2)\n",
    "#     list_suffixe_3 = get_suffixe(list_of_words, 3)\n",
    "\n",
    "    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "mot = 'A'\n",
    "print(mot.isupper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'concatenate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-157212d92847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'eu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'pu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'concatenate'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
