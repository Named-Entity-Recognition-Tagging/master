{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten, \n",
    "    Conv2D,\n",
    "    MaxPooling2D)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', 'Peter', 'Blackburn', 'BRUSSELS', '1996-08-22', 'The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', 'Germany', \"'s\", 'representative']\n",
      "ha []\n",
      "ha []\n",
      "ho []\n",
      "ho []\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Data structure :'Word POS_tag syntactic_chunk_tag named_entity_tag \\n'\n",
    "\n",
    "# train = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.train', 'at')\n",
    "# testa = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.testa')\n",
    "# testb = open(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.testb')\n",
    "  \n",
    "def get_words(path, pos): # Add a 'r' before the path for raw string, pos between 0 and 3 see data structure\n",
    "    data = open(path)\n",
    "    list_words = []\n",
    "    new_data = data.readlines()[2:] # Skip first 2 lines\n",
    "    mylist = list(new_data) # Make the data readable\n",
    "    for i in range(50): # TODO : CHANGES 20 TO len(mylist)\n",
    "        if splitter(mylist[i], ' ', pos): # it return None if the line est empty\n",
    "            list_words.append((splitter(mylist[i], ' ', pos)))\n",
    "#     print(list_words)\n",
    "    return list_words\n",
    "\n",
    "def splitter(line, separator, pos): # pos between 0 and 3 for CONLL2003\n",
    "    if line.isspace(): # Removing blank lines\n",
    "        return\n",
    "    new_sentence = []\n",
    "    for word in line.split(separator):\n",
    "        new_sentence.append(word)\n",
    "    return new_sentence[pos]\n",
    "\n",
    "def get_dictionary(list_of_words): # Return a dict with how many times a word appears\n",
    "    dicti = {}\n",
    "    for word in list_of_words:\n",
    "        if word in dicti:\n",
    "            dicti[word] = dicti[word] + 1\n",
    "        else:\n",
    "            dicti[word] = 1\n",
    "    return dicti\n",
    "\n",
    "def get_reduced_dictionary(dicti, limit): # return a dict with key >= limit\n",
    "    for v, k in list(dicti.items()):\n",
    "        if k <= limit:\n",
    "            del dicti[v]\n",
    "    return dicti\n",
    "\n",
    "def get_prefixe(reduced_dict, length_prefixe):\n",
    "    #TODO : remove all ponctuations signs\n",
    "    list_prefixes = []\n",
    "    for word in reduced_dict:\n",
    "        prefixe = word[:length_prefixe]\n",
    "        list_prefixes.append(prefixe)\n",
    "    print('ha', list_prefixes)\n",
    "    return list_prefixes\n",
    "\n",
    "def get_suffixe(reduced_dict, length_suffixe):\n",
    "    #TODO : remove all ponctuations signs\n",
    "    list_suffixes = []\n",
    "    for word in reduced_dict:\n",
    "        util_position = length_suffixe * (-1)\n",
    "        suffixe = word[util_position:]\n",
    "        list_suffixes.append(suffixe)\n",
    "    print('ho', list_suffixes)\n",
    "    return list_suffixes\n",
    "\n",
    "def get_capital(list_words):\n",
    "    # This function returns 2 lists:\n",
    "    list_capitals = []   # 1/ If the word have a capital\n",
    "    list_first_word = [] # 2/ If the word is the 1st word of the sentence\n",
    "    chara_end_sentence = ['.', '!', '?', '...'] # TODO : Check for '...' if it should be in this list\n",
    "    flag_new_sentence = True\n",
    "    for word in list_words:\n",
    "        letter = word[0] # First 'letter' of the word; can be number or special chara\n",
    "        if letter.isupper():\n",
    "            list_capitals.append(1)\n",
    "            if flag_new_sentence:\n",
    "                list_first_word.append(1)\n",
    "            else:\n",
    "                list_first_word.append(0)\n",
    "        else:\n",
    "            list_capitals.append(0)\n",
    "            list_first_word.append(0)\n",
    "        if letter in chara_end_sentence:\n",
    "            flag_new_sentence = True\n",
    "            continue\n",
    "        flag_new_sentence = False\n",
    "    print(list_capitals, list_first_word)\n",
    "    return (list_capitals, list_first_word)\n",
    "\n",
    "\n",
    "def has_numbers(string):\n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "\n",
    "def get_number(list_words): # Return array if there is a number in the word\n",
    "    list_numbers = []\n",
    "    for word in list_words:\n",
    "        if has_numbers(word):\n",
    "            list_numbers.append(1)\n",
    "        else:\n",
    "            list_numbers.append(0)\n",
    "    print(list_numbers)\n",
    "    return list_numbers\n",
    "\n",
    "def has_middle_dash(string):\n",
    "    return any(char == '-' for char in string)\n",
    "\n",
    "def get_middle_dash(list_words):\n",
    "    list_middle_dashes = []\n",
    "    for word in list_words:\n",
    "        if has_middle_dash(word):\n",
    "            list_middle_dashes.append(1)\n",
    "        else:\n",
    "            list_middle_dashes.append(0)\n",
    "    print(list_middle_dashes)\n",
    "    return list_middle_dashes\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    list_of_words = get_words(r'C:\\Users\\gabgu\\MSA\\PFE\\Perceptron\\master\\Dataset\\eng.train', 0)\n",
    "    print(list_of_words)\n",
    "    dicti = get_dictionary(list_of_words)\n",
    "    reduced_dict = get_reduced_dictionary(dicti, 5)\n",
    "    list_prefixe_2 = get_prefixe(reduced_dict, 2)\n",
    "    list_prefixe_3 = get_prefixe(reduced_dict, 3)\n",
    "    list_suffixe_2 = get_suffixe(reduced_dict, 2)\n",
    "    list_suffixe_3 = get_suffixe(reduced_dict, 3)\n",
    "    list_capitals, list_new_sentence = get_capital(list_of_words)\n",
    "    list_numbers = get_number(list_of_words)\n",
    "    list_middle_dashes = get_middle_dash(list_of_words)\n",
    "    \n",
    "    \n",
    "\n",
    "#TODO REMOVE ALL DOCSTART LINES    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "mot = '2'\n",
    "print(mot.isupper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "bojour\n",
      "popo\n",
      "{'date': 1999, 'bojour': 3, 'popo': 15}\n"
     ]
    }
   ],
   "source": [
    "test={\n",
    "    \"date\": 1999,\n",
    "    \"bojour\": 3, \n",
    "    \"popo\": 15\n",
    "}\n",
    "for x in test:\n",
    "    print(x)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
